{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SES  MMSE  eTIV   nWBV  CDR\n",
      "0    2    27  1987  0.696  0.0\n",
      "1    2    30  2004  0.681  0.0\n",
      "2    0    23  1678  0.736  0.5\n",
      "3    0    28  1738  0.713  0.5\n",
      "4    0    22  1698  0.701  0.5\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset_arranged.csv')\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing empty data with the mean of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_zero = ['SES','MMSE','eTIV','nWBV']\n",
    "\n",
    "for column in replace_zero:\n",
    "    dataset[column] = dataset[column].replace(0,np.nan)\n",
    "    mean = int(dataset[column].mean(skipna=True))\n",
    "    dataset[column] = dataset[column].replace(np.nan,mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,0:4]\n",
    "y = dataset.iloc[:,4]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=11,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olimpia\\Documents\\Python Scripts\\new\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\olimpia\\Documents\\Python Scripts\\new\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\olimpia\\Documents\\Python Scripts\\new\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data from continuous to multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous\n",
      "multiclass\n",
      "continuous\n",
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "lab_enc = LabelEncoder()\n",
    "\n",
    "y_train_encoded = lab_enc.fit_transform(y_train)\n",
    "y_test_encoded = lab_enc.fit_transform(y_test)\n",
    "\n",
    "print(utils.multiclass.type_of_target(y_train))\n",
    "print(utils.multiclass.type_of_target(y_train_encoded))\n",
    "\n",
    "print(utils.multiclass.type_of_target(y_test))\n",
    "print(utils.multiclass.type_of_target(y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=1,p=2,metric='euclidean',weights='uniform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Model\n",
    "\n",
    "classifier.fit(x_train,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred_df = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[38  2  0]\n",
      " [ 6 16  3]\n",
      " [ 0  2  8]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.80      0.64      0.71        25\n",
      "           2       0.73      0.80      0.76        10\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        75\n",
      "   macro avg       0.80      0.80      0.79        75\n",
      "weighted avg       0.82      0.83      0.82        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_encoded,y_pred_df)\n",
    "print(\"Confusion Matrix\\n\\n\",cm)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Evaluating the Model\n",
    "\n",
    "print(metrics.classification_report(y_test_encoded,y_pred_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=  82.67 %\n"
     ]
    }
   ],
   "source": [
    "#Accuracy \n",
    "\n",
    "accuracy = accuracy_score(y_test_encoded,y_pred_df)*100\n",
    "print(\"\\nAccuracy= \",accuracy.round(2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall for micro avg is  82.67 %\n",
      "\n",
      "Recall is macro avg is 79.67 %\n"
     ]
    }
   ],
   "source": [
    "#recall\n",
    "recall= recall_score(y_test_encoded,y_pred_df, average ='micro')*100\n",
    "recall1= recall_score(y_test_encoded,y_pred_df, average ='macro')*100\n",
    "print('\\nRecall for micro avg is ',recall.round(2),'%')\n",
    "print('\\nRecall is macro avg is',recall1.round(2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision for micro is  0.83 %\n",
      "\n",
      " Precision is macro 0.8 %\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "precision = precision_score(y_test_encoded,y_pred_df, average='micro')\n",
    "precision1 = precision_score(y_test_encoded,y_pred_df, average='macro')\n",
    "print(\"\\n Precision for micro is \", precision.round(2),\"%\")\n",
    "print(\"\\n Precision is macro\", precision1.round(2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " F1 measure for micro is  0.83 %\n",
      "\n",
      " F1 measure for micro is  79.26 %\n"
     ]
    }
   ],
   "source": [
    "#f1 score\n",
    "f = f1_score(y_test_encoded,y_pred_df,average= 'micro')\n",
    "fm = f1_score(y_test_encoded,y_pred_df,average= 'macro')*100\n",
    "print(\"\\n F1 measure for micro is \", f.round(2),\"%\")\n",
    "print(\"\\n F1 measure for micro is \", fm.round(2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "\n",
    "classifier1 = KNeighborsClassifier(n_neighbors=1,p=2,metric='manhattan',weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Model\n",
    "\n",
    "classifier1.fit(x_train,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results\n",
    "\n",
    "y_pred1 = classifier1.predict(x_test)\n",
    "y_pred_df1 = pd.DataFrame(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[36  4  0]\n",
      " [ 6 15  4]\n",
      " [ 0  3  7]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        40\n",
      "           1       0.68      0.60      0.64        25\n",
      "           2       0.64      0.70      0.67        10\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        75\n",
      "   macro avg       0.73      0.73      0.73        75\n",
      "weighted avg       0.77      0.77      0.77        75\n",
      "\n",
      "\n",
      "Accuracy=  77.33 %\n",
      "\n",
      "Recall for micro is  77.33 %\n",
      "\n",
      "Recall for macro is  73.33 %\n",
      "\n",
      "Precision for micro is  77.33 %\n",
      "\n",
      "Precision for macro is  72.51 %\n",
      "\n",
      " F1 measure for micro is  0.77 %\n",
      "\n",
      " F1 measure for micro is  72.77 %\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "cm1 = confusion_matrix(y_test_encoded,y_pred_df1)\n",
    "print(\"Confusion Matrix\\n\\n\",cm1)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Evaluating the Model\n",
    "\n",
    "print(metrics.classification_report(y_test_encoded,y_pred_df1))\n",
    "\n",
    "#Accuracy \n",
    "\n",
    "accuracy = accuracy_score(y_test_encoded,y_pred_df1)*100\n",
    "print(\"\\nAccuracy= \",accuracy.round(2),'%')\n",
    "\n",
    "#recall\n",
    "recall= recall_score(y_test_encoded,y_pred_df1, average ='micro')*100\n",
    "recall1= recall_score(y_test_encoded,y_pred_df1, average ='macro')*100\n",
    "print('\\nRecall for micro is ',recall.round(2),'%')\n",
    "print('\\nRecall for macro is ',recall1.round(2),'%')\n",
    "\n",
    "#precision\n",
    "precision = precision_score(y_test_encoded,y_pred_df1, average ='micro')*100\n",
    "precision1 = precision_score(y_test_encoded,y_pred_df1, average ='macro')*100\n",
    "print('\\nPrecision for micro is ',precision.round(2),'%')\n",
    "print('\\nPrecision for macro is ',precision1.round(2),'%')\n",
    "\n",
    "#f1\n",
    "f = f1_score(y_test_encoded,y_pred_df1,average= 'micro')\n",
    "fm = f1_score(y_test_encoded,y_pred_df1,average= 'macro')*100\n",
    "print(\"\\n F1 measure for micro is \", f.round(2),\"%\")\n",
    "print(\"\\n F1 measure for micro is \", fm.round(2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "\n",
    "classifier2 = KNeighborsClassifier(n_neighbors=1,p=2,metric='minkowski',weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Model\n",
    "\n",
    "classifier2.fit(x_train,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results\n",
    "\n",
    "y_pred2 = classifier2.predict(x_test)\n",
    "y_pred_df2 = pd.DataFrame(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      " [[38  2  0]\n",
      " [ 6 16  3]\n",
      " [ 0  2  8]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.80      0.64      0.71        25\n",
      "           2       0.73      0.80      0.76        10\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        75\n",
      "   macro avg       0.80      0.80      0.79        75\n",
      "weighted avg       0.82      0.83      0.82        75\n",
      "\n",
      "\n",
      "Accuracy=  82.67 %\n",
      "\n",
      "Recall for micro is  82.67 %\n",
      "\n",
      "Recall for macro is  79.67 %\n",
      "\n",
      "Precision for micro is  82.67 %\n",
      "\\Precision for macro  is  79.7 %\n",
      "\n",
      " F1 measure for micro is  0.83 %\n",
      "\n",
      " F1 measure for micro is  79.26 %\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "cm3 = confusion_matrix(y_test_encoded,y_pred_df2)\n",
    "print(\"Confusion Matrix\\n\\n\",cm3)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Evaluating the Model\n",
    "\n",
    "print(metrics.classification_report(y_test_encoded,y_pred_df2))\n",
    "\n",
    "#Accuracy \n",
    "\n",
    "accuracy = accuracy_score(y_test_encoded,y_pred_df2)*100\n",
    "print(\"\\nAccuracy= \",accuracy.round(2),'%')\n",
    "\n",
    "#recall\n",
    "recall= recall_score(y_test_encoded,y_pred_df2, average ='micro')*100\n",
    "recall1= recall_score(y_test_encoded,y_pred_df2, average ='macro')*100\n",
    "print('\\nRecall for micro is ',recall.round(2),'%')\n",
    "print('\\nRecall for macro is ',recall1.round(2),'%')\n",
    "\n",
    "#precision\n",
    "precision= precision_score(y_test_encoded,y_pred_df2, average ='micro')*100\n",
    "precision1= precision_score(y_test_encoded,y_pred_df2, average ='macro')*100\n",
    "print('\\nPrecision for micro is ',precision.round(2),'%')\n",
    "print('\\Precision for macro  is ',precision1.round(2),'%')\n",
    "\n",
    "#f1\n",
    "f = f1_score(y_test_encoded,y_pred_df2,average= 'micro')\n",
    "fm = f1_score(y_test_encoded,y_pred_df2,average= 'macro')*100\n",
    "print(\"\\n F1 measure for micro is \", f.round(2),\"%\")\n",
    "print(\"\\n F1 measure for micro is \", fm.round(2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
